# TASTE

This repository provides code for TASTE, a two-phase semantic type detection framework. Semantic type detection can drive a wide range of applications, such as table understanding, data cataloging and search, data quality validation, data transformation, data wrangling, etc. TASTE is particularly effective and efficient in semantic type detection when dealing with a large number of tables from diverse customers in the cloud.

## Environment
You can use `conda` to create a virtual environment and install requirements as follow:

```sh
$ conda create --name taste python=3.6.9
$ conda activate taste
$ pip install -r requirements.txt
```

In addition, you need to set up a MySQL server (8.0.x is preferred).

## Prepare Data

### 1. WikiTable data

Download the following files with the same name from the `data` directory of [this link](https://buckeyemailosu-my.sharepoint.com/:f:/g/personal/deng_595_buckeyemail_osu_edu/EjZWRtslWX9CubQ92jlmNTgB74hxxXszy9BUaXG5OL5F-g?e=HN2qtD) and put them in the `data/wikitable` directory at the root of the project.

```
├── data
    └── wikitable
        ├── train.table_col_type.json
        ├── dev.table_col_type.json
        └── test.table_col_type.json
```

### 2. GitTables data

Download all the zip files from [this link](https://zenodo.org/record/6517052), put them in the `data/gittables/src_zip` directory.

```
├── data
    └── gittables
        ├── src_zip
            ├── abstraction_tables_licensed.zip
            ├── allegro_con_spirito_tables_licensed.parquet
            └── ...
```

And then run `unzip_gittables.sh` to unzip them:
```sh
$ ./unzip_gittables.sh
```

The directory structure after unzipping: 
```
├── data
    └── gittables
        └── unzipped
            ├── abstraction_tables_licensed
                ├── _1.parquet
                └── ...
            ├── allegro_con_spirito_tables_licensed
            └── ...
```

### 3. Pre-trained hybrid model
Download the pre-trained checkpoint from the `checkpoint/pretrained` directory of [this link](https://buckeyemailosu-my.sharepoint.com/:f:/g/personal/deng_595_buckeyemail_osu_edu/EjZWRtslWX9CubQ92jlmNTgB74hxxXszy9BUaXG5OL5F-g?e=HN2qtD) and put it in the `checkpoints/pretrained_hybrid_model` directory at the root of the project.

```
├── checkpoints
    └── pretrained_hybrid_model
        └── pytorch_model.bin 
```

## Run TASTE

### Step1: Construct GitTables-100K datasets (Optional)

Run the following command to construct GitTables-100K datasets:
```sh
python data_process/gittables_selector.py
```
After running successfully, it will generate 3 json files in the data/gittables directory.
```
├── data
    └── gittables
        ├── train.gitables_100k.json
        ├── dev.gitables_100k.json
        └── test.gitables_100k.json
```

### Step2: Fine-tune ADTD Model
Run the following command to fine tune the ADTD Model:
```sh
CUDA_VISIBLE_DEVICES="0"
python finetuning.py \
    --do_train \
    --train_dataset="data/wikitable/train.table_col_type.json" \
    --dev_dataset="data/wikitable/dev.table_col_type.json" \
    --type_vocab="type_vocab/wikitable/type_vocab.txt" \
    --hybrid_model_path="checkpoints/pretrained_hybrid_model/pytorch_model.bin" \
    --output_dir="<adtd_model_dir>" \
    --evaluate_during_training \
    --overwrite_output_dir \
    --use_histogram_feature
```

Parameter explanations:
- `--do_train`: Run training.
- `--train_dataset`: The path of training dataset. Value should be:
    - `"data/wikitable/train.table_col_type.json"` for WikiTable.
    -  `"data/gittables/train.gitables_100k.json"` for GitTables-100k.
- `--dev_dataset`: The path of validation dataset. Value should be:
    -  `"data/wikitable/dev.table_col_type.json"` for WikiTable .
    -  `"data/gittables/dev.gitables_100k.json"` for GitTables-100k.
- `--type_vocab`: The path of type_vocab file. Value should be:
    - `"type_vocab/wikitable/type_vocab.txt"` for WikiTable.
    - `"type_vocab/wikitable/type_vocab_{k}.txt"` for WikiTable-Sk, where {k} indicates the number of types randomly reserved from WikiTable. `type_vocab_{k}.txt` is generated by `type_vocab/vocab_util.py`.
    - `"type_vocab/gittables/type_vocab_1953.txt"` for GitTables-100k. 
- `--hybrid_model_path`: The path of pre-trained hybrid model.
- `--output_dir`: The ADTD model output directory. Different dataset or model setting should specify a different directory.
- `--evaluate_during_training`: Run evaluation during training at each logging step.
- `--overwrite_output_dir`: Overwrite the content of the output directory.
- `--use_histogram_feature`: Use histogram feature to fine-tune model.


### Step3: Build MySQL tables for testing
Run the following command to build MySQL tables for testing:
```sh
python build_mysql_table.py \
    --mysql_host=<mysql_host> \
    --mysql_port=<mysql_port> \
    --mysql_user=<mysql_user> \
    --mysql_password=<mysql_password> \
    --eval_database=<mysql_database> \
    --test_dataset="data/wikitable/test.table_col_type.json"
```
Parameter explanations:
- `--mysql_host`: The hostname or IP address of the MySQL server.
- `--mysql_port`: The port number of the MySQL server.
- `--mysql_user`: The MySQL username for the connection.
- `--mysql_password`: The password associated with the username.
- `--eval_database`: An empty database used to store testing dataset. If it doesn't exist, the program will automatically create it.
- `--test_dataset`: The path of testing dataset. Value should be:
    - `"data/wikitable/test.table_col_type.json"` for WikiTable.
    - `"data/gittables/test.gitables_100k.json"` for GitTables-100k.

### Step4: Evaluation
Run the following command to get evaluation result:
```sh
CUDA_VISIBLE_DEVICES="0"
python evaluation.py \
    --mysql_host=<mysql_host> \
    --mysql_port=<mysql_port> \
    --mysql_user=<mysql_user> \
    --mysql_password=<mysql_password> \
    --eval_database=<mysql_database> \
    --test_dataset="data/wikitable/test.table_col_type.json" \
    --model_dir="<adtd_model_dir>" \
    --type_vocab="type_vocab/wikitable/type_vocab.txt" \
    --use_histogram_feature \
    --threshold_alpha=0.1 \
    --threshold_beta=0.9 \
    --enable_pipeline
```
Parameter explanations:
- `--mysql_host`: The hostname or IP address of the MySQL server.
- `--mysql_port`: The port number of the MySQL server.
- `--mysql_user`: The MySQL username for the connection.
- `--mysql_password`: The password associated with the username.
- `--eval_database`: The database that stores testing dataset. It should be consistent with the same parameter in Step3.
- `--test_dataset`: The path of testing dataset. It should be consistent with the same parameter in Step3. 
- `--model_dir`: The path of fine-tuned ADTD model. It should be consistent with the `--output_dir` in Step2. 
- `--type_vocab`: The path of type_vocab file. It should be consistent with the same parameter in Step2.
- `--use_histogram_feature`: Whether to use histogram feature. It should be consistent with the same parameter in Step2.
- `--threshold_alpha`: The value of threshold α.
- `--threshold_beta`: The value of threshold β.
- `--enable_pipeline`: Enable pipelined execution.

After running successfully, it will print the evaluation results for the selected dataset, including execution time, f1-score and ratio of scanned columns.
